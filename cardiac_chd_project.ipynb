{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN0qmse9xITYgFI6hE6eo3g",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vijay313v/airbnb.eda/blob/main/cardiac_chd_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xeW9PIfJdopw"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix,classification_report,roc_auc_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "qWJ2Dgz5e5SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading dataset\n",
        "cv_df =pd.read_csv(r\"/content/data_cardiovascular_risk.csv\")"
      ],
      "metadata": {
        "id": "ghGXkv1ce5Po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# first 5 observations\n",
        "cv_df.head()"
      ],
      "metadata": {
        "id": "d_RrMZNAfPrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shape of dataset\n",
        "cv_df.shape"
      ],
      "metadata": {
        "id": "JHqdL6MvfPnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3390 observation and 17 columns**"
      ],
      "metadata": {
        "id": "v6YgnkHtLQwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#summary of datset\n",
        "cv_df.info()"
      ],
      "metadata": {
        "id": "rqbHVkUbfPjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for null values\n",
        "cv_df.isnull().sum()"
      ],
      "metadata": {
        "id": "DNquIgZffPg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#statistical description of dataframe\n",
        "cv_df.describe()"
      ],
      "metadata": {
        "id": "fGdmvAxlfPd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking the distribution of the target variable\n",
        "cv_df['TenYearCHD'].value_counts()"
      ],
      "metadata": {
        "id": "uo4Az91BfjY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EDA :**"
      ],
      "metadata": {
        "id": "PbDa05t0frZT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Which sex is most likely to suffer from positive CHD.**"
      ],
      "metadata": {
        "id": "dGlfJmih4pEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis of sex column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['sex'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title('Gender more prone to CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "tv8dkhk5fy7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Being a Male has high chances of CHD compare to Female.**"
      ],
      "metadata": {
        "id": "0urq4XqBL4i2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Smoking effect on CHD**"
      ],
      "metadata": {
        "id": "yqg5QjjnMfxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on cigs per day column\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['cigsPerDay'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title('How much smoking cig per day has risk to CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dplPgxvJfy1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Which Age people has high chances of positive CHD.**"
      ],
      "metadata": {
        "id": "wo8OqrqOPdV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on Age column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['age'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Which Age more prone to CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SniOG349fyw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Age between 47 to 65 has high chances of positive CHD**"
      ],
      "metadata": {
        "id": "f9oLjdzK5e2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Does people taking BPMeds effect on CHD**"
      ],
      "metadata": {
        "id": "MHoTtGwP69VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis on BPMeds\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['BPMeds'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' ')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ccwe6jnmfyuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BPMeds increase the chances of CHD**"
      ],
      "metadata": {
        "id": "L4QIdIth7-0C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How does prevalent Stroke effect in positive CHD factor.**"
      ],
      "metadata": {
        "id": "PO4VUUL68FuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on prevalent stroke column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['prevalentStroke'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' IS prevalent Stroke effect in CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RETX90fufyru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prevalent stroke increases chances of CHD in future.**"
      ],
      "metadata": {
        "id": "1ONGdzwc9bSm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Does Prevalent Hypertension has effect on positive CHD in future**"
      ],
      "metadata": {
        "id": "OlR-lLp-9o0p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis on  prevalent Hpypertension column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['prevalentHyp'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Effect of prevalent Hpypertension on CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2NHelS0fypG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**prevalent Hypertension increases chances of CHD in patients**"
      ],
      "metadata": {
        "id": "ziCe6xam-GbW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Does Diabetes affect the chances of having a positive CHD risk factor**"
      ],
      "metadata": {
        "id": "UD18jrR1-TSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Analysis of Diabetes column\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Create a custom palette\n",
        "my_palette = {0: 'blue', 1: 'orange'}\n",
        "\n",
        "# Create the countplot\n",
        "ax = sns.countplot(x=cv_df['diabetes'], hue=cv_df['TenYearCHD'], palette=my_palette)\n",
        "\n",
        "plt.title(' Effect of Diabetes on CHD')\n",
        "plt.legend(['No Risk', 'At Risk'])\n",
        "\n",
        "# Calculate and add percentage text to the plot\n",
        "total = len(cv_df)\n",
        "for p in ax.patches:\n",
        "    height = p.get_height()\n",
        "    ax.text(p.get_x() + p.get_width() / 2, height + 5, f'{height/total*100:.2f}%', ha='center')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "aAK8qPC8fymT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Diabetes increases chances of CHD**"
      ],
      "metadata": {
        "id": "IpNshfBN-1yf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Cleaning**"
      ],
      "metadata": {
        "id": "DSPYC6-XpZkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for null values\n",
        "cv_df.isnull().sum()"
      ],
      "metadata": {
        "id": "YE2BUJEsfyi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of columns with missing values that have to fill with mean\n",
        "columns_to_fill = ['BPMeds',  'glucose']\n",
        "\n",
        "# Iterate through the columns and fill NaN values with the mean of each column\n",
        "for col in columns_to_fill:\n",
        "    mean_value = cv_df[col].mean()\n",
        "    cv_df[col].fillna(mean_value, inplace=True)"
      ],
      "metadata": {
        "id": "ihYGUNGLpqGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df.dropna(inplace=True)\n",
        "cv_df.drop('id', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "98IeeK6BpqDu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing duplicate values\n",
        "cv_df.drop_duplicates(inplace=True)\n"
      ],
      "metadata": {
        "id": "mRWG4RyGpqA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Outliers**"
      ],
      "metadata": {
        "id": "H48Y04AnqAJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# List with numerical columns that have outliers\n",
        "outlier_cols = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\n",
        "\n",
        "# Visualizing boxplots to find if columns contain outliers\n",
        "plt.figure(figsize=(15, 10))  # Adjust the figsize as needed\n",
        "\n",
        "for index, item in enumerate(outlier_cols):\n",
        "    plt.subplot(2, 4, index + 1)\n",
        "    sns.boxplot(x=cv_df[item])  # Access the column using cv_df[item]\n",
        "    plt.title(item)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6WY3a546pp-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From above boxplot we can infer that these columns have outliers, but in practicality even though they are not normal observations but it is still possible.So,ruling out such possible scenarios can be harmful for our future predictions. Therefore i will allow these outliers to be in dataset.**"
      ],
      "metadata": {
        "id": "5Y_ml57iqXMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Engineering :**"
      ],
      "metadata": {
        "id": "iDPx1IjGqcDR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Encoding :**"
      ],
      "metadata": {
        "id": "Y9hVVBOWqgzC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Machine Learning model work with numerical values therefore categorical columns have to converted/encoded into numerical variables.This process is known as Feature Encoding\n",
        "\n",
        "Here we have two columns that require encoding and they are \"sex\" and \"is_smoking\"."
      ],
      "metadata": {
        "id": "GcwB_Go5qldJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoding the categorical columns\n",
        "cv_df['sex'] = cv_df['sex'].apply(lambda x: 1 if x == 'M' else 0)\n",
        "cv_df['is_smoking'] = cv_df['is_smoking'].apply(lambda x: 1 if x == 'YES' else 0)"
      ],
      "metadata": {
        "id": "m7rx56dJqmi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Grouping columns for better Understanding :**"
      ],
      "metadata": {
        "id": "3Yi48jYaqsja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def smoke_pattern (cigperday:float):\n",
        "  \"\"\"A function that returns the Smoking level\n",
        "     by taking cigarettes per day as an input.\"\"\"\n",
        "\n",
        "  if cigperday==0:                    #Non smoker\n",
        "    return 1\n",
        "  elif cigperday>0 and cigperday<=10:       #Smoker with more than 0 and less than 10 cigs per day\n",
        "    return 2\n",
        "  elif cigperday>10 and cigperday<=20:      #Smoker with more than 10 and less than 20 cigs per day\n",
        "    return 3\n",
        "  elif cigperday>20 and cigperday<=30:      #Smoker with more than 20 and less than 30 cigs per day\n",
        "    return 4\n",
        "  elif cigperday>30 and cigperday<=40:      #Smoker with more than 30 and less than 40 cigs per day\n",
        "    return 5\n",
        "  else:                         #Smoker with more than 40 cigs per day\n",
        "    return 6\n"
      ],
      "metadata": {
        "id": "C92nWKjtwz2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Creating the Smokepattern column\n",
        "cv_df['smoke_pattern'] = cv_df['cigsPerDay'].apply(lambda x: smoke_pattern(x))"
      ],
      "metadata": {
        "id": "7I91aO6gwzwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Removing columns upon whom grouping has been done\n",
        "cv_df.drop(columns={'is_smoking','cigsPerDay'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "tM7mrYnXwztH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cv_df.head()"
      ],
      "metadata": {
        "id": "z-VjKvggwzlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BPlevel**"
      ],
      "metadata": {
        "id": "duDIusrwd63G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a function to assign blood pressure levels\n",
        "def bp_level(row):\n",
        "    if row['sysBP'] < 120 or row['diaBP'] < 80:\n",
        "        return 1  # Normal level\n",
        "    elif (120 <= row['sysBP'] < 130) or row['diaBP'] < 80:\n",
        "        return 2  # Elevated level\n",
        "    elif (130 <= row['sysBP'] < 140) or (80 <= row['diaBP'] < 90):\n",
        "        return 3  # High BP stage 1\n",
        "    elif (140 <= row['sysBP'] < 180) or (90 <= row['diaBP'] < 120):\n",
        "        return 4  # High BP stage 2\n",
        "    else:\n",
        "        return 5  # Hypertensive crisis\n",
        "\n",
        "# Create the 'BPLevel' column using the function\n",
        "cv_df['BPLevel'] = cv_df.apply(bp_level, axis=1)\n",
        "\n",
        "# Remove the 'sysBP' and 'diaBP' columns\n",
        "cv_df.drop(columns=['sysBP', 'diaBP'], inplace=True)\n",
        "\n",
        "# Checking if the 'BPLevel' column is created properly\n",
        "cv_df.head()"
      ],
      "metadata": {
        "id": "Y8281lUtd6zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DiabetesLevel**"
      ],
      "metadata": {
        "id": "rzvoOzUfeUQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to assign diabetes levels\n",
        "def diabetes_level(glucose):\n",
        "    if glucose < 53:\n",
        "        return 1  # Severe Hypoglycemia\n",
        "    elif 53 <= glucose < 70:\n",
        "        return 2  # Hypoglycemia\n",
        "    elif 70 <= glucose < 125:\n",
        "        return 3  # Normal\n",
        "    elif 125 <= glucose < 200:\n",
        "        return 4  # Pre Diabetic\n",
        "    else:\n",
        "        return 5  # Severe Diabetes\n",
        "\n",
        "# Create the 'DiabetesLevel' column using the function\n",
        "cv_df['DiabetesLevel'] = cv_df['glucose'].apply(diabetes_level)\n",
        "\n",
        "# Remove the 'diabetes' and 'glucose' columns\n",
        "cv_df.drop(columns=['diabetes', 'glucose'], inplace=True)\n",
        "\n",
        "# Checking if the 'DiabetesLevel' column is created properly\n",
        "cv_df.head()\n"
      ],
      "metadata": {
        "id": "JbfpRJiBd6we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checking correlation for feature removal:**"
      ],
      "metadata": {
        "id": "SxlODKlxefMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting correlation matrix using sns heatmap\n",
        "corr_matrix= cv_df.corr()\n",
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(corr_matrix,annot=True,cmap='coolwarm')\n",
        "plt.title(\"Correlation between the variables of the dataset\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nkImD_iId6t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**There is no high correlation between majority variables but there for majority of the variables but there is a high correlation between \"prevalentHyp\" and \"BPLevel\". Here i will remove \"prevalentHyp\" because this is somehow direct related with \"BPLevel\" in mediacal terms.**"
      ],
      "metadata": {
        "id": "vL83aRXMeqzS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove columns with high correlation\n",
        "cv_df.drop('prevalentHyp', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "iyT-aW9Ad6nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will reduce variables that do not contribute much in predicting the target variables"
      ],
      "metadata": {
        "id": "gJE3DvTMbuiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Checking the distribution of the data:**"
      ],
      "metadata": {
        "id": "Dyt2KU_rbjZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the independent variables\n",
        "independent_cols=list(set(cv_df.columns)-{'TenYearCHD'})\n"
      ],
      "metadata": {
        "id": "aa61lKR3d6i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n=1\n",
        "plt.figure(figsize=(14,30))\n",
        "for i in independent_cols:\n",
        "  plt.subplot(12,4,n)\n",
        "  n= n+1\n",
        "  sns.distplot(cv_df[i],color='teal')\n",
        "  plt.title(i)\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "eLB0TPc5b-oL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the distribution, there is a high class imbalance for the columns BPMeds and prevalentStroke, so they won't be able to impact the prediction of the target variable much and therefore we'll delete them.\n",
        "\n",
        "From the EDA process we also saw that Education is not a great contributing factor, therefore I'll remove the education column also."
      ],
      "metadata": {
        "id": "x4On6Li98JcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing useless columns\n",
        "cv_df.drop(columns={'BPMeds','prevalentStroke','education'},axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "FQ66tcJOb-id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dealing with class imbalance:**"
      ],
      "metadata": {
        "id": "3yuaylCm8P86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking for class imbalance for the target variable\n",
        "cv_df['TenYearCHD'].value_counts()"
      ],
      "metadata": {
        "id": "bvE8PbOUb-ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the pie chart to check the balance in the dataset.\n",
        "\n",
        "plt.figure(figsize=(7,5), dpi=100)\n",
        "proportion = cv_df['TenYearCHD'].value_counts()\n",
        "labels = ['SAFE','AT RISK']\n",
        "plt.title('Proportion of Safe and at Risk for Target Feature')\n",
        "plt.pie(proportion, explode=(0,0.2),labels=labels, shadow = True, autopct = '%1.1f%%')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "u04MxT5t8ii3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see there is high class Imbalance"
      ],
      "metadata": {
        "id": "TBhY-us1CBer"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Imbalance data **"
      ],
      "metadata": {
        "id": "7XbOWijDm-O_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handling imbalance of target variable using SMOTE(Synthetic Minority Oversampling Technique)"
      ],
      "metadata": {
        "id": "lNgoGg89nHAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset for the independent and dependent variables.\n",
        "X = cv_df.drop('TenYearCHD', axis=1)\n",
        "Y = cv_df['TenYearCHD'].reset_index(drop=True)\n",
        "\n",
        "# Applying the SMOTE technique to solve class imbalance\n",
        "smote = SMOTE(sampling_strategy='minority')\n",
        "X_resampled, Y_resampled = smote.fit_resample(X, Y)\n",
        "\n",
        "# Displaying the first few rows of the resampled independent variables (X_resampled)\n",
        "X_resampled.head()\n"
      ],
      "metadata": {
        "id": "UQ_a9foKb-bH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_resampled.value_counts()"
      ],
      "metadata": {
        "id": "sOWl_pVMnhE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Class imbalance is now removed.**"
      ],
      "metadata": {
        "id": "FRJ50bgAnmcC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting the Data :**"
      ],
      "metadata": {
        "id": "eato7WpCn0g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting the data\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X_resampled,Y_resampled,test_size=0.25,random_state=12)"
      ],
      "metadata": {
        "id": "EmQY9PApnhB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Scaling :**"
      ],
      "metadata": {
        "id": "Bp-mgu3-oCrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Scaling is a technique to standardize the independent features present in the data in a fixed range. It is performed during the data pre-processing to handle highly varying magnitudes or values or units. If feature scaling is not done, then a machine learning algorithm tends to weigh greater values, higher and consider smaller values as the lower values, regardless of the unit of the values."
      ],
      "metadata": {
        "id": "vf_KfHzcoH0k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**StandardScaler is used to resize the distribution of values ​​so that the mean of the observed values ​​is 0 and the standard deviation is 1.**"
      ],
      "metadata": {
        "id": "wdGTZT9BoOZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def scale_numeric_columns(data):\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Scale only the numeric columns\n",
        "    numeric_data = data.select_dtypes(include=['number'])\n",
        "\n",
        "    # Fit and transform the scaler on the numeric data\n",
        "    scaled_data = scaler.fit_transform(numeric_data)\n",
        "\n",
        "    # Create a DataFrame with scaled values and original column names\n",
        "    scaled_df = pd.DataFrame(scaled_data, columns=numeric_data.columns)\n",
        "\n",
        "    return scaled_df"
      ],
      "metadata": {
        "id": "29rEw4Mung-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Scaling the independent dataset\n",
        "scaled_X_train = scale_numeric_columns(X_train)\n",
        "scaled_X_test = scale_numeric_columns(X_test)"
      ],
      "metadata": {
        "id": "SNne1u9zofXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Performance Metrics**"
      ],
      "metadata": {
        "id": "DHhXRlG1oj3n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Different performance metrics are used to evaluate machine learning model. Based on our task we can choose our performance metrics. Since our task is of classification and that too binary class classification, whether client will or will not subscribe for deposits.\n",
        "\n",
        "Here we will be using AUC ROC\n",
        "\n",
        "ROC also known as Receiver Operating Characteristics, shows the performance of binary class classifiers across the range of all possible thresholds plotting between true positive rate and 1-false positive rate.\n",
        "\n",
        "AUC measures the likelihood of two given random points, one from positive and one from negative, the classifier will rank the positive points above negative points. AUC-ROC is popular classification metric that presents the advantage of being independent of false positive or negative points.\n",
        "\n",
        "Secondary Performance Metrics\n",
        "\n",
        "Macro-F1 Score: F1 score is the harmonic mean between Precision and Recall. Macro F1 score is used to know how our model works in overall dataset.\n",
        "\n",
        "Confusion Matrix: This matrix gives the count of true negative, true positive, false positive and false negative data points."
      ],
      "metadata": {
        "id": "kGoO2QOgosWi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def model_evaluator(actual, preds, ml_model, mode):\n",
        "    \"\"\"Evaluate a machine learning model and display metrics.\"\"\"\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(actual, preds)\n",
        "    print(\"Confusion Matrix:\\n\", cm, '\\n')\n",
        "    sns.heatmap(cm, annot=True, cmap='coolwarm', fmt='d')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('Actual Labels')\n",
        "    plt.title(f'Confusion Matrix for {ml_model} on the {mode} set')\n",
        "    plt.show()\n",
        "\n",
        "    # ROC AUC score\n",
        "    roc_auc = roc_auc_score(actual, preds)\n",
        "    print('\\nROC AUC Score:', roc_auc)\n",
        "\n",
        "    # Classification report\n",
        "    print('\\nClassification Report:\\n')\n",
        "    target_names = ['Class 0', 'Class 1']\n",
        "    print(classification_report(actual, preds, target_names=target_names))\n",
        "\n",
        "def model_pipeline(X_train, X_test, Y_train, Y_test, ml_model, param_grid=None, kind='evaluate'):\n",
        "    \"\"\"Train and evaluate machine learning models.\"\"\"\n",
        "    global model\n",
        "\n",
        "    # Logistic Regression\n",
        "    if ml_model == 'Logistic Regression':\n",
        "        model = LogisticRegression(random_state=12)\n",
        "\n",
        "    # Decision Tree, Random Forest, Gradient Boosting\n",
        "    elif ml_model in ['Decision Tree Classifier', 'Random Forest Classifier', 'Gradient Boosting Classifier']:\n",
        "        model_init = {\n",
        "            'Decision Tree Classifier': DecisionTreeClassifier(),\n",
        "            'Random Forest Classifier': RandomForestClassifier(),\n",
        "            'Gradient Boosting Classifier': GradientBoostingClassifier()\n",
        "        }[ml_model]\n",
        "\n",
        "        gs_model = GridSearchCV(estimator=model_init, param_grid=param_grid, cv=5, scoring='roc_auc', verbose=True)\n",
        "        gs_model.fit(X_train, Y_train)\n",
        "\n",
        "        print(\"Best parameters for\", ml_model, \":\", gs_model.best_params_, '\\n')\n",
        "\n",
        "        model = gs_model.best_estimator_\n",
        "    else:\n",
        "        print(\"Enter correct model name: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, or Gradient Boosting Classifier.\")\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "\n",
        "    train_predictions = model.predict(X_train)\n",
        "    test_predictions = model.predict(X_test)\n",
        "\n",
        "    if kind == 'evaluate':\n",
        "        print(\"1. Train set evaluation:\")\n",
        "        model_evaluator(Y_train, train_predictions, ml_model, 'Train')\n",
        "        print(\"\\n2. Test set evaluation:\")\n",
        "        model_evaluator(Y_test, test_predictions, ml_model, 'Test')\n",
        "\n",
        "    elif kind == 'model_explainability':\n",
        "        return model\n",
        "\n",
        "# Example usage:\n",
        "# Assuming X_train, X_test, Y_train, and Y_test are defined earlier\n",
        "\n",
        "\n",
        "param_grid_dt = {\n",
        "    'max_depth': [4, 6, 8, 10],\n",
        "    'min_samples_split': [5, 10, 20, 30, 40, 50],\n",
        "    'min_samples_leaf': [5, 10, 15, 20]\n",
        "}\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [50, 65, 80, 95, 120],\n",
        "    'max_depth': [3, 5, 7, 9, 10]\n",
        "}\n",
        "\n",
        "param_grid_gb = {\n",
        "    'n_estimators': [80, 100],\n",
        "    'max_depth': [5, 7, 8],\n",
        "    'learning_rate': [0.001, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tMLybQYqDCf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Logistic regression')"
      ],
      "metadata": {
        "id": "IPVXeuVT7WaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Decision Tree Classifier', param_grid=param_grid_dt, kind='evaluate')"
      ],
      "metadata": {
        "id": "5pYdTqtTEYT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Random Forest', param_grid=param_grid_rf, kind='evaluate')"
      ],
      "metadata": {
        "id": "rSMEFUI8EgyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Gradient Boosting Classifier', param_grid=param_grid_gb, kind='evaluate')"
      ],
      "metadata": {
        "id": "qiNQ6zxiDCc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model explainability:**"
      ],
      "metadata": {
        "id": "jfx7hP6bwhuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the shap library\n",
        "!pip install shap"
      ],
      "metadata": {
        "id": "iY8-lIZYwrgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing the SHAP library\n",
        "import shap"
      ],
      "metadata": {
        "id": "kxS7Td9ow9Xr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating an object for the logistic regression model\n",
        "lr_classifier = model_pipeline(X_train,X_test,Y_train,Y_test, ml_model='Logistic Regression',kind='model_explainability')"
      ],
      "metadata": {
        "id": "C04OdGaUxGec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the shap summary plot\n",
        "explainer_shap = shap.Explainer(model=model, masker=X_train)\n",
        "shap_values = explainer_shap.shap_values(X_train)\n",
        "shap.summary_plot(shap_values,X_train,feature_names=X.columns)"
      ],
      "metadata": {
        "id": "HnCcgleryftt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_importance(model):\n",
        "  features = X.columns\n",
        "  importances = model.feature_importances_\n",
        "  indices = np.argsort(importances)\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.title('Feature Importance')\n",
        "  plt.barh(range(len(indices)), importances[indices], color='blue', align='center')\n",
        "  plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
        "  plt.xlabel('Relative Importance')\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "IT6VVR4oyi0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Dt_clf=model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='Decision Tree Classifier',\n",
        "                          param_grid={'max_depth': [4, 6, 8, 10],\n",
        "                                      'min_samples_split': [5, 10, 20, 30, 40, 50],\n",
        "                                      'min_samples_leaf': [5, 10, 15, 20]},\n",
        "                          kind='model_explainability')"
      ],
      "metadata": {
        "id": "Vaekbi7-z0gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting the feature importance for Decision tree classifier\n",
        "feature_importance(Dt_clf)"
      ],
      "metadata": {
        "id": "Rf351jbMovsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_clf = model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='RandomForestClassifier',\n",
        "                           param_grid={'n_estimators': [50, 65, 80, 95, 120],\n",
        "                                       'max_depth': [3, 5, 7, 9, 10]},\n",
        "                           kind='model_explainability')"
      ],
      "metadata": {
        "id": "NvYJhvJEpYzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance(rf_clf)"
      ],
      "metadata": {
        "id": "Ec8X65s8pB8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gb_clf = model_pipeline(X_train, X_test, Y_train, Y_test, ml_model='GradientBoostingClassifier',\n",
        "                           param_grid={'n_estimators': [80, 100],\n",
        "                                       'max_depth': [5, 7, 8],\n",
        "                                       'learning_rate':[0.001, 0.01, 0.05]},\n",
        "                           kind='model_explainability')"
      ],
      "metadata": {
        "id": "8oUcjEVHpp_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feature_importance(gb_clf)"
      ],
      "metadata": {
        "id": "-RarkOYip6kw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Summary :**"
      ],
      "metadata": {
        "id": "RGdUrDrIoRXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Males are having high chances of CHD.\n",
        "2.   Smoking increases CHD chances.\n",
        "3.   Age between 47 to 65 years increase risk factor for CHD.\n",
        "4.   People taking BPMeds also has high chances of CHD.\n",
        "5.   Prevalent Stroke increases risk factors for CHD.\n",
        "6.   Prevalent Hypertension also increases CHD chances in future.\n",
        "7.   Diabetes also impact on positive CHD in future.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "peq2B8RVoalT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results from ML model :**"
      ],
      "metadata": {
        "id": "0c51SBZepvNc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Logistic regression gives a ROCAUC score of 0.69022 on the testing set.\n",
        "2.   Decision tree model gives a ROCAUC score of 0.68495 on the testing set.\n",
        "     This is the worst performing model\n",
        "3.   Random Forest Classifier model gives a ROCAUC score of 0.77373 on the\n",
        "     testing set.\n",
        "4.   Gradient Boosting Classifier model gives a ROCAUC score of 0.8255 on the\n",
        "     testing set.This is the best performing model\n",
        "5.   Model explainability has been achieved by SHAP library's summary plot and\n",
        "     an attribute called feature_importances_ of the tree based algorithms.\n",
        "6.   Total cholestrol and age are the two most important factors to predict the\n",
        "     CHD risk factor.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Dr0APiMqDaS"
      }
    }
  ]
}